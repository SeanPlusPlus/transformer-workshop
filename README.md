# Transformer Workshop

Transformer Workshop is an exploratory project focused on hands-on learning and experimentation with transformer-based models. The journey began with a goal: to train a custom sentiment analysis model and document the process as a detailed tutorial. Over time, this project evolved into a comprehensive workshop, covering not just fine-tuning pre-trained models but also experimenting with Hugging Face pipelines and broader concepts in natural language processing (NLP).

## Backstory

I started with this prompt:

<img width="827" alt="image" src="https://github.com/user-attachments/assets/36b6b209-fc83-40ef-8d96-e01f4ad609fb">

... And iteratively built out these scripts as well as this corresponding document ...

It's been a pretty cool first for me - using ChatGPT to autogenerate an iterative tutorial with functioning code and documentation!

---

## Project Goals

1. Train a Sentiment Analysis Model:

    - Fine-tune a pre-trained transformer model (distilbert-base-uncased) for IMDb sentiment analysis.
    - Build a clear, step-by-step tutorial to guide others through the process.

2. Explore Text Generation:

    - Experiment with pre-trained models for tasks like text summarization, question answering, and more.
    - Understand how different transformer architectures are optimized for various NLP tasks.

3. Document Insights:

    - Capture high-level notes and comparisons, such as the differences between BERT and LLaMA.
    - Organize everything into a cohesive, shareable resource.
